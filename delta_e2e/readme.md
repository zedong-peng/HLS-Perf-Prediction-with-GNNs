# SimpleDifferentialGNN Architecture

This document summarizes the network stack defined in `train_e2e.py` for the `SimpleDifferentialGNN` model. The model supports multiple graph backbones (`gcn`, `gin`, `rgcn`, `fast_rgcn`) but the high-level layout of layers is consistent.

å¥½çš„ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªæ›´**ç®€æ´ã€é¢å‘ç»„ä¼šæ±‡æŠ¥çš„ç‰ˆæœ¬**ï¼ˆå»æ‰ `std`ã€`p90`ï¼Œä»…ä¿ç•™æœ€æœ‰ä¿¡æ¯é‡çš„æŒ‡æ ‡ï¼šmean / median / p95 / maxï¼‰ï¼š

---

# ğŸ”§ ForgeHLS Resource Statistics (Simplified)

| Metric      | Type   |     Mean | Median |     p95 |       Max |
| :---------- | :----- | -------: | -----: | ------: | --------: |
| **DSP**     | Kernel |     17.7 |      0 |      35 |      3.6K |
|             | Design |     91.8 |      0 |      80 |      225K |
|             | Î”      |     74.0 |      0 |      35 |      225K |
| **LUT**     | Kernel |     2.7K |    194 |    8.2K |      394K |
|             | Design |    13.4K |    577 |   29.3K |     14.8M |
|             | Î”      |    10.6K |    236 |   18.5K |     14.8M |
| **FF**      | Kernel |    1.39K |     45 |   5.97K |       55K |
|             | Design |    6.81K |    110 |   17.0K |      6.5M |
|             | Î”      |    5.42K |     25 |   9.45K |      6.5M |
| **Latency** | Kernel | 4.15Ã—10â¸ |   1.0K | 5.2Ã—10â¶ |  6.0Ã—10Â¹â° |
|             | Design | 6.80Ã—10â¸ |   1.0K | 2.4Ã—10â· |  1.1Ã—10Â¹Â¹ |
|             | Î”      | 2.65Ã—10â¸ |     âˆ’1 | 6.0Ã—10â¶ | 1.05Ã—10Â¹Â¹ |

---

### ğŸ’¡ Quick Takeaways

* **Resources (DSP/LUT/FF)** increase **3â€“10Ã—** after HLS optimization (Design vs Kernel).
* **Î” distributions** are long-tailed but mostly small; many cases remain near-zero (sparse changes).
* **Latency** spans over **10 orders of magnitude**, dominated by a few extremely long-running designs â€” normalization or log-scale modeling is essential.
* **Median values near 0** show most kernels are lightweight; heavy outliers dominate resource variance.


## Core Encoder
- **Node Encoder:** `Linear(node_dim â†’ hidden_dim)` projects raw node features before message passing.

## Message Passing Stack (repeated `num_layers` times)
For each layer index `i` in `0 â€¦ num_layers-1`:
- **Graph Convolution:** one of
  - `GCNConv(hidden_dim, hidden_dim)` when `gnn_type == 'gcn'`
  - `GINConv(MLP(hidden_dim â†’ hidden_dim â†’ hidden_dim))` when `gnn_type == 'gin'`
  - `RGCNConv(hidden_dim, hidden_dim, num_relations, num_bases=30)` when `gnn_type == 'rgcn'`
  - `FastRGCNConv(hidden_dim, hidden_dim, num_relations, num_bases=30)` when `gnn_type == 'fast_rgcn'`
- **Normalization:** `LayerNorm(hidden_dim)`
- **Activation:** `ReLU` on all but the final message-passing block.

## Graph Readout
- **Pooling:** `global_add_pool` reduces node embeddings to a graph-level representation (summing over the batch indices).

## Prediction Heads
- **Kernel Head** *(optional; only when `differential=True` and `kernel_baseline='learned'`)*:
  - `Linear(hidden_dim â†’ hidden_dim)` + `ReLU`
  - `Dropout(p=dropout)`
  - `Linear(hidden_dim â†’ hidden_dim/2)` + `ReLU`
  - `Linear(hidden_dim/2 â†’ 1)`
- **Delta Head** *(when `differential=True`)*:
  - Input dimension `4 * hidden_dim` built from `[kernel_repr, design_repr, design_repr - kernel_repr, design_repr * kernel_repr]`
  - Same MLP stack as the kernel head.
- **Design Head** *(when `differential=False`)*:
  - Same MLP stack as the kernel head but fed with the pooled design representation directly.

## Output Logic
- In differential mode, the final design prediction is `kernel_pred + delta_pred`.
- In direct mode, the design head output is returned as the prediction.

All linear layers are initialized with Xavier-uniform weights and zero biases.
