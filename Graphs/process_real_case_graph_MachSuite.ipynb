{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'MachSuite'\n",
    "data_of_designs_json_path = '/home/user/zedongpeng/workspace/HLSBatchProcessor/csv/data_of_designs_.json'\n",
    "designs_dir = f'/home/user/zedongpeng/workspace/HLSBatchProcessor/data/designs/MachSuite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "from os import path\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to extract graphs from adb files, and to save into json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rtl_hash_table(root):\n",
    "    \"\"\"\n",
    "    param: \n",
    "        root: the root of the adb file\n",
    "    return:\n",
    "        rtl_table: This file returns a hash table of resources and the rtlNames.\n",
    "    \"\"\"\n",
    "    all_rtl = root.findall('*/res/*/item')\n",
    "    rtl_table = {}\n",
    "    if_add = False\n",
    "    rep = re.compile(' \\(.*\\)')\n",
    "    for i in all_rtl:\n",
    "        res_table = {}\n",
    "        rtl_name = i.find('first').text\n",
    "        rtl_res = i.find('second')\n",
    "        if rtl_name not in rtl_table.keys():\n",
    "            for res in rtl_res.iter('item'):\n",
    "                try:\n",
    "                    res_name = res.findall('first')[0].text\n",
    "                    res_num = res.findall('second')[0].text\n",
    "                except BaseException:\n",
    "                    # print('The RTL $',rtl_name,'& does not contain any resource info.')\n",
    "                    break\n",
    "                else:\n",
    "                    if res_name in res_considered:\n",
    "                        res_table[res_name] = res_num\n",
    "                        if_add = True\n",
    "        if if_add:\n",
    "            rtl_table[re.sub(rep, '', rtl_name)] = res_table\n",
    "        if_add = False\n",
    "    return rtl_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### parse adb files into graphs (in json)\n",
    "res_considered = ['FF', 'LUT', 'DSP']\n",
    "\n",
    "def parse_xml_into_graph_single(xml_file):\n",
    "    prefix = ''\n",
    "    G = nx.DiGraph()\n",
    "    parser = et.parse(xml_file)\n",
    "    root = parser.getroot()\n",
    "    cdfg = root.findall('*/cdfg')[0]\n",
    "\n",
    "    # rtl hash table\n",
    "    rtl_res_table = get_rtl_hash_table(root)\n",
    "\n",
    "    ### find edges and build the graph\n",
    "    #print(\"Adding Edges\")\n",
    "    edge_id_max = -1\n",
    "    for edges in cdfg.iter('edges'):\n",
    "        for edge in edges.iter('item'):\n",
    "            source = edge.find('source_obj').text\n",
    "            sink = edge.find('sink_obj').text\n",
    "            edge_id = edge.find('id').text\n",
    "            edge_id_max = max(int(edge_id), edge_id_max)\n",
    "            is_back_edge = edge.find('is_back_edge').text\n",
    "            edge_type = edge.find('edge_type').text\n",
    "            G.add_edges_from([(prefix + source, prefix + sink, {'edge_name': prefix + edge_id, 'is_back_edge': is_back_edge, 'edge_type': edge_type})])\n",
    "\n",
    "    ### add node attributes\n",
    "    #print(\"Adding Nodes\")\n",
    "    for nodes in cdfg.iter('nodes'):\n",
    "        for node in nodes.findall('item'):\n",
    "            node_id = node.findall('*/*/id')[0].text\n",
    "            node_name = prefix + node_id\n",
    "        \n",
    "            if node_name not in G.nodes():\n",
    "                #print('Node %s (type: nodes) not in the graph' % node_name)\n",
    "                op_code = node.findall('opcode')[0].text\n",
    "                if op_code == 'ret':\n",
    "                    G.add_node(node_name)\n",
    "                    G.nodes[node_name]['node_name'] = node_name\n",
    "                    G.nodes[node_name]['category']='nodes'\n",
    "                    G.nodes[node_name]['bitwidth'] = node.findall('*/bitwidth')[0].text\n",
    "                    G.nodes[node_name]['opcode'] = node.findall('opcode')[0].text\n",
    "                    G.nodes[node_name]['m_Display'] = node.findall('m_Display')[0].text\n",
    "                    G.nodes[node_name]['m_isOnCriticalPath'] = node.findall('m_isOnCriticalPath')[0].text\n",
    "                    G.nodes[node_name]['m_isStartOfPath'] = node.findall('m_isStartOfPath')[0].text\n",
    "                    G.nodes[node_name]['m_delay'] = node.findall('m_delay')[0].text\n",
    "                    G.nodes[node_name]['m_topoIndex'] = node.findall('m_topoIndex')[0].text\n",
    "                    G.nodes[node_name]['m_isLCDNode'] = node.findall('m_isLCDNode')[0].text\n",
    "                    G.nodes[node_name]['m_clusterGroupNumber'] = node.findall('m_clusterGroupNumber')[0].text\n",
    "                    G.nodes[node_name]['type'] = node.findall('*/*/type')[0].text\n",
    "                    G.nodes[node_name]['LUT'] = '0'\n",
    "                    G.nodes[node_name]['FF'] = '0'\n",
    "                    G.nodes[node_name]['DSP'] = '0'\n",
    "                continue\n",
    "\n",
    "            G.nodes[node_name]['node_name'] = node_name        \n",
    "            G.nodes[node_name]['category'] = 'nodes'\n",
    "            G.nodes[node_name]['bitwidth'] = node.findall('*/bitwidth')[0].text\n",
    "            G.nodes[node_name]['opcode'] = node.findall('opcode')[0].text\n",
    "            G.nodes[node_name]['m_Display'] = node.findall('m_Display')[0].text\n",
    "            G.nodes[node_name]['m_isOnCriticalPath'] = node.findall('m_isOnCriticalPath')[0].text\n",
    "            G.nodes[node_name]['m_isStartOfPath'] = node.findall('m_isStartOfPath')[0].text\n",
    "            G.nodes[node_name]['m_delay'] = node.findall('m_delay')[0].text\n",
    "            G.nodes[node_name]['m_topoIndex'] = node.findall('m_topoIndex')[0].text\n",
    "            G.nodes[node_name]['m_isLCDNode'] = node.findall('m_isLCDNode')[0].text\n",
    "            G.nodes[node_name]['m_clusterGroupNumber'] = node.findall('m_clusterGroupNumber')[0].text\n",
    "            G.nodes[node_name]['type'] = node.findall('*/*/type')[0].text\n",
    "            # rtl info below\n",
    "            # every nodes has the three features, so we initilize them as 0.\n",
    "            G.nodes[node_name]['LUT'] = '0'\n",
    "            G.nodes[node_name]['FF'] = '0'\n",
    "            G.nodes[node_name]['DSP'] = '0'\n",
    "            t_rtlname = node.findall('*/*/rtlName')[0].text\n",
    "            if t_rtlname != None:\n",
    "                # if this nodes has a rtlName info\n",
    "                if t_rtlname in rtl_res_table.keys():\n",
    "                    # if this rtlName has corresponding resources info\n",
    "                    # print(t_rtlname, '+++++++++++', rtl_res_table[t_rtlname])\n",
    "                    res_name = rtl_res_table[t_rtlname].keys()\n",
    "                    for i in res_name:\n",
    "                        # rewrite the initial number with the actual number\n",
    "                        G.nodes[node_name][i] = rtl_res_table[t_rtlname][i]\n",
    "\n",
    "    ## blocks are for control signals\n",
    "    for nodes in cdfg.iter('blocks'):\n",
    "        for node in nodes.findall('item'):\n",
    "            node_id = node.findall('*/id')[0].text\n",
    "            node_name = prefix + node_id\n",
    "\n",
    "            if node_name not in G.nodes():\n",
    "                #print('Node %s (type: blocks) not in the graph' % node_name)\n",
    "                continue\n",
    "            G.nodes[node_name]['node_name'] = node_name        \n",
    "            G.nodes[node_name]['category'] = 'blocks'\n",
    "            G.nodes[node_name]['type'] = node.findall('*/type')[0].text\n",
    "    \n",
    "    ## ports are function arguments \n",
    "    for nodes in cdfg.iter('ports'):\n",
    "        for node in nodes.findall('item'):\n",
    "            node_id = node.findall('*/*/id')[0].text\n",
    "            node_name = prefix + node_id\n",
    "\n",
    "            if node_name not in G.nodes():\n",
    "                #print('Node %s (type: ports) not in the graph' % node_name)\n",
    "                continue\n",
    "            G.nodes[node_name]['node_name'] = node_name        \n",
    "            G.nodes[node_name]['category'] = 'ports'\n",
    "            G.nodes[node_name]['type'] = node.findall('*/*/type')[0].text\n",
    "            G.nodes[node_name]['bitwidth'] = node.findall('*/bitwidth')[0].text\n",
    "            G.nodes[node_name]['direction'] = node.findall('direction')[0].text\n",
    "            G.nodes[node_name]['if_type'] = node.findall('if_type')[0].text\n",
    "            G.nodes[node_name]['array_size'] = node.findall('array_size')[0].text\n",
    "\n",
    "    ## no need to keep consts as nodes in the graph\n",
    "    ## remove to reduce the graph size\n",
    "    for nodes in cdfg.iter('consts'):\n",
    "        for node in nodes.findall('item'):\n",
    "            node_id = node.findall('*/*/id')[0].text\n",
    "            node_name = prefix + node_id\n",
    "\n",
    "            if node_name not in G.nodes():\n",
    "                #print('Node %s (type: consts) not in the graph' % node_name)\n",
    "                continue\n",
    "            for v in G.neighbors(node_name):\n",
    "                G.nodes[v]['const'] = node_name\n",
    "                G.nodes[v]['const-bitwidth'] = node.findall('*/bitwidth')[0].text\n",
    "            # remove the const node\n",
    "            G.remove_node(node_name)\n",
    "            #print(\"const node %s removed\" % node_name)\n",
    "\n",
    "    #edge_list = list(G.edges)\n",
    "    #print(edge_list)\n",
    "    #node_list = list(G.nodes)\n",
    "    #print(node_list)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save one graph into json\n",
    "def json_save(G, fname):\n",
    "    f = open(fname + '.json', 'w')\n",
    "    G_dict = dict(nodes=[[n, G.nodes[n]] for n in G.nodes()], \\\n",
    "                  edges=[(e[0], e[1], G.edges[e]) for e in G.edges()])\n",
    "    json.dump(G_dict, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the graphs into json\n",
    "def json_save_graphs(Gs, fname):\n",
    "    f = open(fname + '.json', 'w')\n",
    "    G_dict = dict()\n",
    "    G_dict['nodes'] = []\n",
    "    G_dict['edges'] = []\n",
    "    for G in Gs:\n",
    "        for n in G.nodes():\n",
    "            G_dict['nodes'].append([n, G.nodes[n]])\n",
    "        for e in G.edges():\n",
    "            G_dict['edges'].append((e[0], e[1], G.edges[e]))\n",
    "    json.dump(G_dict, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read the actual resource\n",
    "def get_real_perf(fname):\n",
    "    f = open(fname + '.json', 'r')\n",
    "    d = json.load(f)\n",
    "    f.close()\n",
    "    DSP=d['DSP']\n",
    "    LUT=d['LUT']\n",
    "    FF=d['FF']\n",
    "\n",
    "    return DSP, LUT, FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "\n",
    "def find_cpp_c_files(search_dir, max_depth=5):\n",
    "    \"\"\"\n",
    "    在指定目录及其子目录（最大深度 max_depth）中搜索 .cpp 和 .c 文件。\n",
    "    \n",
    "    :param search_dir: 要搜索的目录\n",
    "    :param max_depth: 最大搜索深度（默认值为5）\n",
    "    :return: 包含 .cpp 和 .c 文件路径的列表\n",
    "    \"\"\"\n",
    "    cpp_c_files = []\n",
    "    \n",
    "    # 使用列表推导式快速收集所有目录\n",
    "    all_dirs = []\n",
    "    for root, dirs, _ in os.walk(search_dir):\n",
    "        # 过滤 .autopilot 目录\n",
    "        if '.autopilot' in dirs:\n",
    "            dirs.remove('.autopilot')\n",
    "        # 计算当前目录的深度\n",
    "        rel_path = os.path.relpath(root, search_dir)\n",
    "        depth = 0 if rel_path == \".\" else rel_path.count(os.sep) + 1\n",
    "        \n",
    "        if depth < max_depth:\n",
    "            all_dirs.append(root)\n",
    "    \n",
    "    # 使用tqdm显示进度\n",
    "    for root in tqdm.tqdm(all_dirs, desc=\"搜索目录\"):\n",
    "        # 直接扩展列表而不是逐个添加\n",
    "        cpp_c_files.extend([os.path.join(root, file) for file in os.listdir(root) \n",
    "                           if file.endswith(('.cpp', '.c'))])\n",
    "    \n",
    "    return cpp_c_files\n",
    "\n",
    "def process_single_file(c_file, designs_dir):\n",
    "    \"\"\"处理单个C/C++文件并生成对应的.adb文件\"\"\"\n",
    "    try:\n",
    "        # 获取绝对路径\n",
    "        abs_path = os.path.abspath(c_file)\n",
    "        # 解析路径获取source_name, kernel_name, design_id\n",
    "        path_parts = abs_path.split(os.sep)\n",
    "        if len(path_parts) < 4:\n",
    "            return []\n",
    "            \n",
    "        file_name = os.path.splitext(path_parts[-1])[0]\n",
    "        design_id = path_parts[-2]\n",
    "        kernel_name = path_parts[-3]\n",
    "        source_name = path_parts[-4]\n",
    "        \n",
    "        # 构建新的文件名前缀\n",
    "        prefix = f\"{source_name}-{kernel_name}-{design_id}-\"\n",
    "        \n",
    "        # 查找对应的.adb文件\n",
    "        adb_dir = os.path.dirname(c_file)\n",
    "        \n",
    "        # 在c_file所在路径的./**多级子目录找 .adb文件 注意最多一个后缀\n",
    "        adb_files = []\n",
    "        for root, dirs, files in os.walk(adb_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.adb') and len(file.split('.')) == 2:\n",
    "                    adb_files.append(os.path.join(root, file))\n",
    "        \n",
    "        # 去除重复项\n",
    "        adb_files = list(set(adb_files))\n",
    "\n",
    "        # 创建目标目录（提前创建以避免多线程冲突）\n",
    "        target_dir = os.path.join('real_case', source_name + '_adb')\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        \n",
    "        # 批量复制文件\n",
    "        for adb_file in adb_files:\n",
    "            original_adb_name = os.path.basename(adb_file)\n",
    "            new_adb_name = f\"{prefix}{original_adb_name}\"\n",
    "            target_path = os.path.join(target_dir, new_adb_name)\n",
    "            shutil.copy2(adb_file, target_path)\n",
    "            \n",
    "        return []\n",
    "    except Exception as e:\n",
    "        return [f\"处理 {c_file} 时出错: {str(e)}\"]\n",
    "\n",
    "def process_c_files_to_adb(designs_dir, c_files):\n",
    "    \"\"\"\n",
    "    处理C/C++文件并生成对应的.adb文件，使用多进程和多线程加速\n",
    "    \n",
    "    Args:\n",
    "        designs_dir: 设计文件根目录\n",
    "        c_files: C/C++文件列表\n",
    "    \"\"\"\n",
    "    # 创建所有可能需要的目标目录，避免线程冲突\n",
    "    os.makedirs('real_case', exist_ok=True)\n",
    "    \n",
    "    # 计算最佳工作进程数\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    workers = min(cpu_count * 2, 64)  # 使用更多的工作线程\n",
    "    \n",
    "    # 创建进度条\n",
    "    pbar = tqdm.tqdm(total=len(c_files), desc=\"处理文件\")\n",
    "    \n",
    "    # 使用线程池加速处理\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # 提交所有任务\n",
    "        futures = [executor.submit(process_single_file, c_file, designs_dir) for c_file in c_files]\n",
    "        \n",
    "        # 处理结果\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                results = future.result()\n",
    "                # 只在有错误时输出\n",
    "                if results:\n",
    "                    print(f\"处理文件出现 {len(results)} 个错误\")\n",
    "            except Exception as e:\n",
    "                print(f\"处理文件时发生异常: {str(e)}\")\n",
    "            finally:\n",
    "                pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "\n",
    "# 主执行代码\n",
    "print(\"开始搜索C/C++文件...\")\n",
    "c_files = find_cpp_c_files(designs_dir)\n",
    "print(f\"找到 {len(c_files)} 个C/C++文件，开始处理...\")\n",
    "\n",
    "# 统计设计数量\n",
    "design_count = {}\n",
    "for c_file in c_files:\n",
    "    try:\n",
    "        path_parts = os.path.abspath(c_file).split(os.sep)\n",
    "        if len(path_parts) >= 4:\n",
    "            source_name = path_parts[-4]\n",
    "            kernel_name = path_parts[-3]\n",
    "            design_id = path_parts[-2]\n",
    "            key = f\"{source_name}-{kernel_name}\"\n",
    "            if key not in design_count:\n",
    "                design_count[key] = set()\n",
    "            design_count[key].add(design_id)\n",
    "    except Exception as e:\n",
    "        print(f\"统计设计时出错: {str(e)}\")\n",
    "\n",
    "# 打印统计结果\n",
    "print(\"\\n设计数量统计:\")\n",
    "total_designs = 0\n",
    "for key, designs in design_count.items():\n",
    "    design_num = len(designs)\n",
    "    total_designs += design_num\n",
    "    print(f\"  {key}: {design_num}个设计\")\n",
    "print(f\"总计: {total_designs}个设计\\n\")\n",
    "\n",
    "# 批量处理以提高效率，增加批次大小\n",
    "batch_size = 2000  # 增加批次大小\n",
    "total_batches = (len(c_files) + batch_size - 1) // batch_size\n",
    "\n",
    "# 预先创建所有可能的目标目录\n",
    "source_names = set()\n",
    "for c_file in c_files:\n",
    "    try:\n",
    "        path_parts = os.path.abspath(c_file).split(os.sep)\n",
    "        if len(path_parts) >= 4:\n",
    "            source_names.add(path_parts[-4])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for source_name in source_names:\n",
    "    os.makedirs(os.path.join('real_case', source_name + '_adb'), exist_ok=True)\n",
    "\n",
    "# 处理批次\n",
    "for i in range(total_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(c_files))\n",
    "    batch = c_files[start_idx:end_idx]\n",
    "    print(f\"处理批次 {i+1}/{total_batches}，文件数量: {len(batch)}\")\n",
    "    process_c_files_to_adb(designs_dir, batch)\n",
    "\n",
    "print(\"处理完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### post hls data\n",
    "df = pd.read_json(data_of_designs_json_path, orient='records', lines=True)\n",
    "for index, row in df.iterrows():\n",
    "    source_name = row['source_name']\n",
    "    kernel_name = row['algo_name']\n",
    "    design_id = row['design_id']\n",
    "    dsp = row['DSP']\n",
    "    lut = row['LUT']\n",
    "    ff = row['FF']\n",
    "    save_path = os.path.join('real_case', f'{dataset_name}', f'posthls_{source_name}-{kernel_name}-{design_id}.json')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump({\"DSP\": dsp, \"LUT\": lut, \"FF\": ff}, f)\n",
    "    print(f\"Saved {save_path}\")\n",
    "    print(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = f'{dataset_name}'\n",
    "graph_dir = f'{dataset_name}_adb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get subgraphs in one application\n",
    "graph_mapping = dict()\n",
    "for adb_file in glob.glob('real_case/' + graph_dir + '*.adb'):\n",
    "    _, _, file_name = adb_file.split('/')\n",
    "    # 获取最后一个-之前和之后的部分\n",
    "    parts = file_name.rsplit('-', 1)\n",
    "    fname = parts[0]  # 最后一个-之前的所有内容\n",
    "    func_name = parts[1]  # 最后一个-之后的内容\n",
    "    if fname not in graph_mapping:\n",
    "        graph_mapping[fname] = [func_name]\n",
    "    else:\n",
    "        graph_mapping[fname].append(func_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_max_node_id(node_string):\n",
    "    node_array=[]\n",
    "    for n in node_string:\n",
    "        node_array.append(int(n))\n",
    "    max_id=max(node_array)\n",
    "    return max_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 最终阶段：将图保存为json文件\n",
    "os.makedirs(os.path.join('real_case', result_dir), exist_ok=True)\n",
    "\n",
    "# 使用多进程并行处理\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import tqdm\n",
    "\n",
    "def process_single_file(adb_file, max_id=0):\n",
    "    try:\n",
    "        g = parse_xml_into_graph_single(adb_file)\n",
    "        if max_id > 0:\n",
    "            # relabel nodes\n",
    "            mapping = {n:str(int(n)+max_id) for n in g.nodes}\n",
    "            g = nx.relabel_nodes(g, mapping)\n",
    "        return g\n",
    "    except Exception as e:\n",
    "        print(f\"处理文件 {adb_file} 时出错: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_fname(fname, graph_dir, result_dir):\n",
    "    try:\n",
    "        graph_num = len(graph_mapping[fname])\n",
    "        \n",
    "        # 预先获取所有匹配的文件列表\n",
    "        adb_files = glob.glob('real_case/' + graph_dir + fname + '-*')\n",
    "        \n",
    "        if graph_num > 1:\n",
    "            max_id = 0\n",
    "            G = []\n",
    "            for adb_file in adb_files:\n",
    "                g = process_single_file(adb_file, max_id)\n",
    "                if g is not None:\n",
    "                    G.append(g)\n",
    "                    max_id = check_max_node_id(g.nodes) + 1\n",
    "            if G:\n",
    "                json_save_graphs(G, 'real_case/' + result_dir + '/' + fname)\n",
    "        else:\n",
    "            if adb_files:\n",
    "                g = process_single_file(adb_files[0])\n",
    "                if g is not None:\n",
    "                    json_save(g, 'real_case/' + result_dir + '/' + fname)\n",
    "        return fname\n",
    "    except Exception as e:\n",
    "        print(f\"处理 {fname} 时出错: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 使用进程池并行处理\n",
    "total_files = len(graph_mapping.keys())\n",
    "print(f\"开始处理总共 {total_files} 个文件...\")\n",
    "\n",
    "with Pool() as pool:\n",
    "    process_func = partial(process_fname, graph_dir=graph_dir, result_dir=result_dir)\n",
    "    results = []\n",
    "    for result in tqdm.tqdm(pool.imap_unordered(process_func, list(graph_mapping.keys())), total=total_files):\n",
    "        results.append(result)\n",
    "\n",
    "# 统计成功和失败的文件数\n",
    "successful = [r for r in results if r is not None]\n",
    "print(f\"处理完成! 成功: {len(successful)}/{total_files}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to process graphs into dataset format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### features for numerical rtl resource\n",
    "\n",
    "allowable_features = {\n",
    "    'node_category' : ['nodes', 'blocks', 'ports', 'misc'], \n",
    "    'bitwidth' : list(range(0, 256)) + ['misc'], \n",
    "    'opcode_category' : ['terminator','binary_unary', 'bitwise', 'conversion','memory','aggregate','other','misc'], \n",
    "    'possible_opcode_list' : [\n",
    "        'br', 'ret', 'switch',\n",
    "        'add', 'dadd', 'fadd', 'sub', 'dsub', 'fsub', 'mul', 'dmul', 'fmul', 'udiv', 'ddiv', 'fdiv', 'sdiv', 'urem', 'srem', 'frem', 'dexp', 'dsqrt',\n",
    "        'shl', 'lshr', 'ashr', 'and', 'xor', 'or',\n",
    "        'uitofp', 'sitofp', 'uitodp', 'sitodp', 'bitconcatenate', 'bitcast', 'zext', 'sext', 'fpext', 'trunc', 'fptrunc',\n",
    "        'extractvalue', 'insertvalue',\n",
    "        'alloca', 'load', 'store', 'read', 'write', 'getelementptr',\n",
    "        'phi', 'call', 'icmp', 'dcmp', 'fcmp', 'select', 'bitselect', 'partselect', 'mux', 'dacc',\n",
    "        'misc'\n",
    "    ],\n",
    "    'possible_is_start_of_path': [0, 1, 'misc'],\n",
    "    'possible_is_LCDnode':[0, 1, 'misc'],\n",
    "    'possible_cluster_group_num': [-1] + list(range(0, 256)) + ['misc'],\n",
    "    'LUT': list(range(0, 1000)) + ['misc'],\n",
    "    'DSP': list(range(0, 11)) + ['misc'],\n",
    "    'FF': list(range(0, 1000)) + ['misc'],\n",
    "    \n",
    "    'possible_edge_type_list' : [1, 2, 3, 'misc'], \n",
    "    'possible_is_back_edge': [0, 1],\n",
    "}\n",
    "\n",
    "def safe_index(l, e):\n",
    "    \"\"\"\n",
    "    Return index of element e in list l. If e is not present, return the last index\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return l.index(e)\n",
    "    except:\n",
    "        return len(l) - 1\n",
    "\n",
    "def opcode_type(opcode):\n",
    "    if opcode in {'br', 'ret', 'switch'}:\n",
    "        t='terminator'\n",
    "    if opcode in {'add', 'dadd', 'fadd', 'sub', 'dsub', 'fsub', 'mul', 'dmul', 'fmul', 'udiv', 'ddiv', 'fdiv', 'sdiv', 'urem', 'srem', 'frem', 'dexp', 'dsqrt'}:\n",
    "        t='binary_unary'\n",
    "    if opcode in {'shl', 'lshr', 'ashr', 'and', 'xor', 'or'}:\n",
    "        t='bitwise'\n",
    "    if opcode in {'uitofp', 'sitofp', 'uitodp', 'sitodp', 'bitconcatenate', 'bitcast', 'zext', 'sext', 'fpext', 'trunc', 'fptrunc'}:\n",
    "        t='conversion'\n",
    "    if opcode in {'alloca', 'load', 'store', 'read', 'write', 'getelementptr'}:\n",
    "        t='memory'\n",
    "    if opcode in {'extractvalue', 'insertvalue'}:\n",
    "        t='aggregate'\n",
    "    if opcode in {'phi', 'call', 'icmp', 'dcmp', 'fcmp', 'select', 'bitselect', 'partselect', 'mux', 'dacc'}:\n",
    "        t='other'\n",
    "    return t\n",
    "\n",
    "\n",
    "\n",
    "def node_to_feature_vector(node):\n",
    "    \"\"\"\n",
    "    Converts node object to feature list of indices\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "\n",
    "    if node=={}:\n",
    "        node_feature = [\n",
    "                len(allowable_features['node_category'])-1,\n",
    "                len(allowable_features['bitwidth'])-1,\n",
    "                len(allowable_features['opcode_category'])-1,\n",
    "                len(allowable_features['possible_opcode_list'])-1,\n",
    "                len(allowable_features['possible_is_start_of_path'])-1,\n",
    "                len(allowable_features['possible_is_LCDnode'])-1,\n",
    "                len(allowable_features['possible_cluster_group_num'])-1,\n",
    "                len(allowable_features['LUT'])-1,\n",
    "                len(allowable_features['DSP'])-1,\n",
    "                len(allowable_features['FF'])-1\n",
    "                ]\n",
    "        return node_feature\n",
    "        \n",
    "    if node['category']=='nodes':\n",
    "        node_feature = [\n",
    "                safe_index(allowable_features['node_category'], node['category']),\n",
    "                safe_index(allowable_features['bitwidth'], int(node['bitwidth'])),\n",
    "                safe_index(allowable_features['opcode_category'], opcode_type(node['opcode'])),\n",
    "                safe_index(allowable_features['possible_opcode_list'], node['opcode']),\n",
    "                safe_index(allowable_features['possible_is_start_of_path'], int(node['m_isStartOfPath'])),\n",
    "                safe_index(allowable_features['possible_is_LCDnode'], int(node['m_isLCDNode'])),\n",
    "                safe_index(allowable_features['possible_cluster_group_num'], int(node['m_clusterGroupNumber'])),\n",
    "                safe_index(allowable_features['LUT'], int(node['LUT'])),\n",
    "                safe_index(allowable_features['DSP'], int(node['DSP'])),\n",
    "                safe_index(allowable_features['FF'], int(node['FF']))\n",
    "                ]\n",
    "    elif node['category']=='ports':\n",
    "        node_feature = [\n",
    "                safe_index(allowable_features['node_category'], node['category']),\n",
    "                safe_index(allowable_features['bitwidth'], int(node['bitwidth'])),\n",
    "                len(allowable_features['opcode_category'])-1,\n",
    "                len(allowable_features['possible_opcode_list'])-1,\n",
    "                len(allowable_features['possible_is_start_of_path'])-1,\n",
    "                len(allowable_features['possible_is_LCDnode'])-1,\n",
    "                len(allowable_features['possible_cluster_group_num'])-1,\n",
    "                len(allowable_features['LUT'])-1,\n",
    "                len(allowable_features['DSP'])-1,\n",
    "                len(allowable_features['FF'])-1\n",
    "                ]\n",
    "    elif node['category']=='blocks':\n",
    "        node_feature = [\n",
    "                safe_index(allowable_features['node_category'], node['category']),\n",
    "                len(allowable_features['bitwidth'])-1,\n",
    "                len(allowable_features['opcode_category'])-1,\n",
    "                len(allowable_features['possible_opcode_list'])-1,\n",
    "                len(allowable_features['possible_is_start_of_path'])-1,\n",
    "                len(allowable_features['possible_is_LCDnode'])-1,\n",
    "                len(allowable_features['possible_cluster_group_num'])-1,\n",
    "                len(allowable_features['LUT'])-1,\n",
    "                len(allowable_features['DSP'])-1,\n",
    "                len(allowable_features['FF'])-1\n",
    "                ]\n",
    "    return node_feature\n",
    "\n",
    "def get_node_feature_dims():\n",
    "    return list(map(len, [\n",
    "        allowable_features['node_category'],\n",
    "        allowable_features['bitwidth'],\n",
    "        allowable_features['opcode_category'],\n",
    "        allowable_features['possible_opcode_list'],\n",
    "        allowable_features['possible_is_start_of_path'],\n",
    "        allowable_features['possible_is_LCDnode'],\n",
    "        allowable_features['possible_cluster_group_num'],\n",
    "        allowable_features['LUT'],\n",
    "        allowable_features['DSP'],\n",
    "        allowable_features['FF'],\n",
    "        ]))\n",
    "\n",
    "\n",
    "def edge_to_feature_vector(edge):\n",
    "    \"\"\"\n",
    "    Converts edge to feature list of indices\n",
    "    :return: list\n",
    "    \"\"\"\n",
    "    bond_feature = [\n",
    "                safe_index(allowable_features['possible_edge_type_list'], int(edge['edge_type'])),\n",
    "                allowable_features['possible_is_back_edge'].index(int(edge['is_back_edge']))\n",
    "            ]\n",
    "    return bond_feature\n",
    "\n",
    "def get_edge_feature_dims():\n",
    "    return list(map(len, [\n",
    "        allowable_features['possible_edge_type_list'],\n",
    "        allowable_features['possible_is_back_edge']\n",
    "        ]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result_dir='PolyBench/'\n",
    "#prefix='polybench_'\n",
    "# result_dir='CHStone/'\n",
    "# prefix='chstone_'\n",
    "result_dir=f'{dataset_name}/'\n",
    "prefix='posthls_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### graphs in json transformed into csv format\n",
    "graph_mapping_list = []\n",
    "num_node_list = []\n",
    "num_edge_list = []\n",
    "\n",
    "DSP = []\n",
    "LUT = []\n",
    "FF = []\n",
    "\n",
    "node_feat = []\n",
    "edge_list = []\n",
    "edge_feat = []\n",
    "\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "total_files = len(glob.glob('real_case/' + result_dir + prefix + '*.json'))\n",
    "print(f\"总文件数: {total_files}\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "for perf_file in tqdm(glob.glob('real_case/' + result_dir + prefix + '*.json'), desc=\"处理文件\"):\n",
    "    _, _, file_name = perf_file.split('/')\n",
    "    graph_name = file_name.replace(prefix,'')\n",
    "\n",
    "    try:\n",
    "        print(f'real_case/'+result_dir + graph_name)\n",
    "        f = open('real_case/'+result_dir + graph_name, 'r')\n",
    "        d = json.load(f)\n",
    "        f.close()\n",
    "        nodes=d['nodes']\n",
    "        edges=d['edges']\n",
    "\n",
    "        try:\n",
    "            dsp, lut, ff = get_real_perf(perf_file.replace('.json',''))\n",
    "            \n",
    "            num_node_list.append(len(nodes))\n",
    "            num_edge_list.append(len(edges))\n",
    "            graph_mapping_list.append(result_dir + graph_name)\n",
    "            \n",
    "            DSP.append(dsp)\n",
    "            LUT.append(lut/1000)\n",
    "            FF.append(ff/1000)\n",
    "\n",
    "            node_index_map = dict() # map the node name to the index\n",
    "            index = 0\n",
    "\n",
    "            for n in nodes:\n",
    "                if n[0] not in node_index_map:\n",
    "                    node_index_map[n[0]] = index\n",
    "                node_feat.append(node_to_feature_vector(n[1]))\n",
    "                index = index + 1\n",
    "            \n",
    "            for e in edges:\n",
    "                source = node_index_map[e[0]]\n",
    "                sink = node_index_map[e[1]]\n",
    "                edge_list.append([source,sink])\n",
    "                edge_feat.append(edge_to_feature_vector(e[2]))\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "    except Exception as e:\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "\n",
    "print(f\"处理完成: {processed_count}/{total_files} 文件\")\n",
    "print(f\"跳过: {skipped_count}/{total_files} 文件 ({skipped_count/total_files*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save graphs into csv files\n",
    "\n",
    "ds_dir = f'{dataset_name}_ds'                \n",
    "save_dir = 'real_case/' + ds_dir + '/' # the directory to save real cases, three benchmarks are saved separately in this stage\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "mapping = pd.DataFrame({'orignal code':graph_mapping_list , 'DSP' : DSP , 'LUT' : LUT, 'FF' : FF})\n",
    "NODE_num = pd.DataFrame(num_node_list) # number of nodes in each graph \n",
    "NODE = pd.DataFrame(node_feat) # node features\n",
    "EDGE_num = pd.DataFrame(num_edge_list) # number of edges in each graph\n",
    "EDGE_list = pd.DataFrame(edge_list) # edge (source, end)\n",
    "EDGE_feat = pd.DataFrame(edge_feat) # edge features\n",
    "\n",
    "graph_label_dsp = pd.DataFrame(DSP)\n",
    "graph_label_lut = pd.DataFrame(LUT)\n",
    "graph_label_ff = pd.DataFrame(FF)\n",
    "\n",
    "# save into csv files\n",
    "mapping.to_csv(save_dir + 'mapping.csv', index = False)\n",
    "NODE_num.to_csv(save_dir + 'num-node-list.csv', index = False, header = False)\n",
    "NODE.to_csv(save_dir + 'node-feat.csv', index = False, header = False)\n",
    "\n",
    "EDGE_num.to_csv(save_dir + 'num-edge-list.csv', index = False, header = False)\n",
    "EDGE_list.to_csv(save_dir + 'edge.csv', index = False, header=False)\n",
    "EDGE_feat.to_csv(save_dir + 'edge-feat.csv', index = False, header = False)\n",
    "\n",
    "graph_label_dsp.to_csv(save_dir + 'graph-label-dsp.csv', index = False, header = False)\n",
    "graph_label_lut.to_csv(save_dir + 'graph-label-lut.csv', index = False, header = False)\n",
    "graph_label_ff.to_csv(save_dir + 'graph-label-ff.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to merge real-case benchmarks with synthetic cdfg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge all three real case with synthetic cdfg\n",
    "# read real-case benchmarks\n",
    "case_dir_all=[\n",
    "    # 'real_case/CHStone_ds/',\n",
    "    # 'real_case/PolyBench_ds/',\n",
    "    f'real_case/{dataset_name}_ds/'\n",
    "]\n",
    "\n",
    "mapping_1 = []\n",
    "edge_feat_1 = []\n",
    "edge_1 = []\n",
    "node_1 = []\n",
    "\n",
    "dsp_1 = []\n",
    "lut_1 = []\n",
    "ff_1 = []\n",
    "cp_1 = []\n",
    "\n",
    "node_num_1 = []\n",
    "edge_num_1 = []\n",
    "\n",
    "\n",
    "for case_dir in case_dir_all:\n",
    "    mapping_1 += pd.read_csv(case_dir + 'mapping.csv').values.tolist()\n",
    "    edge_feat_1 += pd.read_csv(case_dir + 'edge-feat.csv', header = None).values.tolist()\n",
    "    edge_1 += pd.read_csv(case_dir + 'edge.csv', header = None).values.tolist()\n",
    "    node_1 += pd.read_csv(case_dir + 'node-feat.csv', header = None).values.tolist()\n",
    "\n",
    "    dsp_1 += pd.read_csv(case_dir + 'graph-label-dsp.csv', header = None).values.tolist()\n",
    "    lut_1 += pd.read_csv(case_dir + 'graph-label-lut.csv', header = None).values.tolist()\n",
    "    ff_1 += pd.read_csv(case_dir + 'graph-label-ff.csv', header = None).values.tolist()\n",
    "\n",
    "    node_num_1 += pd.read_csv(case_dir + 'num-node-list.csv', header = None).values.tolist()\n",
    "    edge_num_1 += pd.read_csv(case_dir + 'num-edge-list.csv', header = None).values.tolist()\n",
    "\n",
    "# merge together\n",
    "DSP = dsp_1\n",
    "LUT = lut_1\n",
    "FF = ff_1\n",
    "\n",
    "graph_mapping_list = mapping_1\n",
    "num_node_list = node_num_1\n",
    "num_edge_list = edge_num_1\n",
    "\n",
    "node_feat = node_1\n",
    "edge_list = edge_1\n",
    "edge_feat = edge_feat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save merged dataset\n",
    "\n",
    "save_dir = f'real_case/dataset_ready_for_GNN_{dataset_name}/'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "mapping = pd.DataFrame({'orignal code' : graph_mapping_list, 'DSP' : DSP, 'LUT' : LUT, 'FF' : FF})\n",
    "NODE_num = pd.DataFrame(num_node_list)\n",
    "EDGE_num = pd.DataFrame(num_edge_list)\n",
    "\n",
    "graph_label_dsp = pd.DataFrame(DSP)\n",
    "graph_label_lut = pd.DataFrame(LUT)\n",
    "graph_label_ff = pd.DataFrame(FF)\n",
    "\n",
    "NODE = pd.DataFrame(node_feat)\n",
    "EDGE_list = pd.DataFrame(edge_list)\n",
    "EDGE_feat = pd.DataFrame(edge_feat)\n",
    "\n",
    "\n",
    "mapping.to_csv(save_dir + 'mapping.csv', index = False)\n",
    "NODE_num.to_csv(save_dir + 'num-node-list.csv', index = False, header = False)\n",
    "EDGE_num.to_csv(save_dir + 'num-edge-list.csv', index = False, header = False)\n",
    "\n",
    "graph_label_dsp.to_csv(save_dir + 'graph-label-dsp.csv', index = False, header = False)\n",
    "graph_label_lut.to_csv(save_dir + 'graph-label-lut.csv', index = False, header = False)\n",
    "graph_label_ff.to_csv(save_dir + 'graph-label-ff.csv', index = False, header = False)\n",
    "\n",
    "NODE.to_csv(save_dir + 'node-feat.csv', index = False, header = False)\n",
    "EDGE_list.to_csv(save_dir + 'edge.csv', index = False, header = False)\n",
    "EDGE_feat.to_csv(save_dir + 'edge-feat.csv', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to generate training/valid/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for training set\n",
    "from sklearn import model_selection\n",
    "\n",
    "basis = len(pd.read_csv(f'{save_dir}/graph-label-dsp.csv', header=None))\n",
    "print(basis)\n",
    "\n",
    "# 将基础数据集分割为训练集(80%)、验证集(10%)和测试集(10%)\n",
    "indices = [i for i in range(basis)]\n",
    "train_indices, temp_indices = model_selection.train_test_split(indices, train_size=0.8, random_state=42)\n",
    "valid_indices, test_indices = model_selection.train_test_split(temp_indices, train_size=0.5, random_state=42)\n",
    "\n",
    "# 保存训练集\n",
    "train_list = pd.DataFrame(sorted(train_indices))\n",
    "train_list.to_csv(save_dir + 'train.csv', index=False, header=False)\n",
    "\n",
    "# 保存验证集\n",
    "valid_list = pd.DataFrame(sorted(valid_indices))\n",
    "valid_list.to_csv(save_dir + 'valid.csv', index=False, header=False)\n",
    "\n",
    "# 保存测试集\n",
    "test_list = pd.DataFrame(sorted(test_indices))\n",
    "test_list.to_csv(save_dir + 'test.csv', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circuitnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
